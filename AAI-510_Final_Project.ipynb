{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "da0e2e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import skorch\n",
    "import kagglehub\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from skorch import NeuralNetBinaryClassifier\n",
    "from skorch.dataset import ValidSplit\n",
    "from models import MLPNet\n",
    "from sklearn.base import clone\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb850b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.12)\n",
      "Path to dataset files: C:\\Users\\seanp\\.cache\\kagglehub\\datasets\\meruvulikith\\190k-spam-ham-email-dataset-for-classification\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"meruvulikith/190k-spam-ham-email-dataset-for-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa47b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "source": [
    "# Checking for GPU availability\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d8cb286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (193850, 2)\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset\n",
    "df = pd.read_csv(\"spam_Emails_data.csv\")\n",
    "\n",
    "# Dropping rows where label or text is missing\n",
    "df.dropna(subset=['label', 'text'], inplace=True)\n",
    "\n",
    "# Printing size of dataset\n",
    "print(f\"Shape of dataset: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ad030d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataset: (193850, 2)\n"
     ]
    }
   ],
   "source": [
    "# Filtering out rows that aren't labeled correctly\n",
    "df['label'] = df['label'].str.strip().str.lower()\n",
    "df = df[df['label'].isin(['spam', 'ham'])]\n",
    "\n",
    "# Encoding labels as binary\n",
    "df['label'] = df['label'].map({'spam': 1, 'ham': 0})\n",
    "\n",
    "# Printing size of dataset\n",
    "print(f\"Shape of dataset: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "342c3226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some features for the dataset\n",
    "\n",
    "# Email length\n",
    "df['email_length'] = df['text'].apply(len)\n",
    "\n",
    "# Number of exclamation points\n",
    "df['exclamation_points'] = df['text'].str.count('!')\n",
    "\n",
    "# Number of capital letters\n",
    "df['capital_letters'] = df['text'].apply(lambda x: sum(1 for c in x.split() if c.isupper()))\n",
    "\n",
    "# Adding more features here to try and improve performance\n",
    "\n",
    "# Word count\n",
    "df['word_count'] = df['text'].str.split().apply(len)\n",
    "\n",
    "# Puncuation count\n",
    "df['punctuation_count'] = df['text'].apply(lambda x: sum(1 for c in x if c in string.punctuation))\n",
    "\n",
    "# Digit ratio\n",
    "df['digit_ratio'] = df['text'].apply(lambda x: sum(1 for c in x if c.isdigit()) / len(x) if len(x) > 0 else 0)\n",
    "\n",
    "# HTML presence\n",
    "df['has_html'] = df['text'].str.contains(r'<[^>]+>', regex=True).astype(int)\n",
    "\n",
    "# URL presence\n",
    "df['has_url'] = df['text'].str.contains(r'http[s]?://', regex=True).astype(int)\n",
    "\n",
    "# URL count\n",
    "df['num_urls'] = df['text'].str.count(r'http[s]?://')\n",
    "\n",
    "# Email count\n",
    "df['num_emails'] = df['text'].str.count(r'\\b[\\w.-]+?@\\w+?\\.\\w+?\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd9a44d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a9ef55b2c240cca56b963a6c6bc8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3029 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading pretrained sentence embedding model\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2') # experimenting with a few different options here\n",
    "embedder = embedder.to('cuda')\n",
    "\n",
    "# Generating embeddings\n",
    "embeddings = embedder.encode(\n",
    "    df['text'].tolist(),\n",
    "    batch_size=64,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# Normalizing and reshaping engineered features\n",
    "engineered_features = df[['email_length', 'exclamation_points', 'capital_letters', 'word_count', 'punctuation_count', 'digit_ratio', 'has_html', 'has_url', 'num_urls', 'num_emails']]\n",
    "scaler = StandardScaler()\n",
    "engineered_scaled = scaler.fit_transform(engineered_features)\n",
    "\n",
    "# Combining embeddings with engineered features\n",
    "x = np.hstack((embeddings, engineered_scaled))\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd74d5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d07d5797",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training several different models and comparing their performance\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100)\n",
    "}\n",
    "\n",
    "# Utilizing RandomSearchCV to find optimal parameters for each model\n",
    "param_random = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10],\n",
    "        'penalty': ['l2'],\n",
    "        'solver': ['liblinear']\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [50, 100, 150],\n",
    "        'max_depth': [5, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aaa271b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomSearchCV for Logistic Regression\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 4 is smaller than n_iter=10. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Logistic Regression: {'solver': 'liblinear', 'penalty': 'l2', 'C': 10}\n",
      "\n",
      "Logistic Regression Performance after tuning:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9438    0.9435    0.9437     20530\n",
      "           1     0.9364    0.9368    0.9366     18240\n",
      "\n",
      "    accuracy                         0.9403     38770\n",
      "   macro avg     0.9401    0.9401    0.9401     38770\n",
      "weighted avg     0.9403    0.9403    0.9403     38770\n",
      "\n",
      "Running RandomSearchCV for Random Forest\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters for Random Forest: {'n_estimators': 150, 'min_samples_split': 5, 'max_features': 'sqrt', 'max_depth': 20}\n",
      "\n",
      "Random Forest Performance after tuning:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9481    0.9684    0.9581     20530\n",
      "           1     0.9635    0.9404    0.9518     18240\n",
      "\n",
      "    accuracy                         0.9552     38770\n",
      "   macro avg     0.9558    0.9544    0.9550     38770\n",
      "weighted avg     0.9554    0.9552    0.9552     38770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    \n",
    "    print(f\"Running RandomSearchCV for {name}\")\n",
    "    random = RandomizedSearchCV(\n",
    "        model, \n",
    "        param_random[name], \n",
    "        cv=3, \n",
    "        n_iter=10, \n",
    "        scoring='f1', \n",
    "        verbose=1, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    random.fit(x_train, y_train)\n",
    "    \n",
    "    best_model = random.best_estimator_\n",
    "    best_models[name] = best_model\n",
    "    \n",
    "    print(f\"Best parameters for {name}: {random.best_params_}\")\n",
    "    \n",
    "    y_pred = best_model.predict(x_test)\n",
    "    print(f\"\\n{name} Performance after tuning:\\n\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80170e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best parameters for MLP Neural Net: {'module__hidden_layer_sizes': (128, 64), 'module__dropout': 0.3, 'max_epochs': 20, 'lr': 0.001, 'batch_size': 128}\n",
      "\n",
      "MLP Neural Net Performance after tuning:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9813    0.9779    0.9796     20530\n",
      "           1     0.9752    0.9790    0.9771     18240\n",
      "\n",
      "    accuracy                         0.9784     38770\n",
      "   macro avg     0.9782    0.9784    0.9783     38770\n",
      "weighted avg     0.9784    0.9784    0.9784     38770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Handling MLP separately with GPU\n",
    "\n",
    "# Wrapping in skorch for use with PyTorch\n",
    "net = NeuralNetBinaryClassifier(\n",
    "    module=MLPNet,\n",
    "    module__input_dim=x_train.shape[1],\n",
    "    max_epochs=20,\n",
    "    train_split=ValidSplit(0.2),\n",
    "    callbacks=[skorch.callbacks.EarlyStopping(patience=5)],\n",
    "    lr=0.001,\n",
    "    batch_size=128,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    criterion=nn.BCEWithLogitsLoss,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Defining randomized search parameters\n",
    "mlp_param_random = {\n",
    "    'module__hidden_layer_sizes': [(64,), (128,), (128, 64)],\n",
    "    'module__dropout': [0.3, 0.5],\n",
    "    'lr': [0.001, 0.01],\n",
    "    'max_epochs': [10, 20],\n",
    "    'batch_size': [32, 64, 128]\n",
    "}\n",
    "\n",
    "# Running RandomizedSearchCV on GPU-backed model to search wider range quicker\n",
    "random = RandomizedSearchCV(net, mlp_param_random, cv=3, n_iter=10, scoring='f1', verbose=1, n_jobs=1)\n",
    "random.fit(x_train.astype('float32'), y_train.astype('float32'))\n",
    "\n",
    "best_model = random.best_estimator_\n",
    "best_models['MLP Neural Net'] = best_model\n",
    "\n",
    "y_pred = best_model.predict(x_test.astype('float32'))\n",
    "\n",
    "print(f\"Best parameters for MLP Neural Net: {random.best_params_}\")\n",
    "print(\"\\nMLP Neural Net Performance after tuning:\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0a5410c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\seanp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\skorch\\net.py:2261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cuda_attrs = torch.load(f, **load_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked Model Performance:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.9826    0.9811    0.9819     20530\n",
      "         1.0     0.9788    0.9805    0.9796     18240\n",
      "\n",
      "    accuracy                         0.9808     38770\n",
      "   macro avg     0.9807    0.9808    0.9807     38770\n",
      "weighted avg     0.9808    0.9808    0.9808     38770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Wrapper for skorch model\n",
    "class SkorchFloat32Wrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, skorch_model):\n",
    "        self.skorch_model = skorch_model\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = self.label_encoder.fit_transform(y)\n",
    "        self.classes_ = self.label_encoder.classes_\n",
    "        y = y.astype(np.float32)\n",
    "        self.skorch_model.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        preds = self.skorch_model.predict(X)\n",
    "        return self.label_encoder.inverse_transform(preds.astype(int))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.skorch_model.predict_proba(X)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {'skorch_model': self.skorch_model}\n",
    "\n",
    "    def set_params(self, **params):\n",
    "        self.skorch_model = params['skorch_model']\n",
    "        return self\n",
    "\n",
    "# Testing model stacking with the optimal models from above\n",
    "base_learners = [\n",
    "    ('lr', best_models['Logistic Regression']),\n",
    "    ('rf', best_models['Random Forest']),\n",
    "    ('mlp', SkorchFloat32Wrapper(best_models['MLP Neural Net']))\n",
    "]\n",
    "\n",
    "# Using Logistic Regression model as the meta-model\n",
    "meta_learner = LogisticRegression()\n",
    "\n",
    "stacked_model = StackingClassifier(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    passthrough=True\n",
    ")\n",
    "\n",
    "x_train = x_train.astype(np.float32)\n",
    "x_test = x_test.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "stacked_model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = stacked_model.predict(x_test)\n",
    "print(\"Stacked Model Performance:\\n\")\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
