{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPhzj9QX16neCdxnsfNo4FZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seanpaz478/AAI-510-Final-Project-Group7/blob/main/USD%2C_ML%2C_Final_Project%2C_Stacking%2C_LGBM_%26_RFC_%3E_LogReg_(Yaakov_Sternberg).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import os\n",
        "import re\n",
        "import io\n",
        "import csv\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import joblib\n",
        "import string\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import lightgbm as lgb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "from google.colab import files, drive\n",
        "from IPython.display import display, HTML\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    accuracy_score,\n",
        "    matthews_corrcoef,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    precision_recall_curve,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    log_loss,\n",
        "    average_precision_score\n",
        ")\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    StratifiedShuffleSplit,\n",
        "    GridSearchCV,\n",
        "    RandomizedSearchCV,\n",
        "    cross_val_predict\n",
        ")\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# NLTK\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "id": "OavzaY3sD5UE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GLOBAL CONFIGURATION & ENVIRONMENT SETUP"
      ],
      "metadata": {
        "id": "ez8O3EJwj6BA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Settings\n",
        "warnings.filterwarnings('ignore')\n",
        "tqdm.pandas()\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Download NLTK stuff\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoHrCL_aj4Ae",
        "outputId": "ebeff512-af1a-49c1-919e-b44589a3c13d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREPROCESSING"
      ],
      "metadata": {
        "id": "8s5xeV4PhkZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATTERNS = {\n",
        "    \"url\": re.compile(r\"http[s]?://\\S+\"),\n",
        "    \"email\": re.compile(r\"\\S+@\\S+\"),\n",
        "    \"phone\": re.compile(r\"\\b(?:\\d{3}[-.\\s]?)?\\d{3}[-.\\s]?\\d{4}\\b\"),\n",
        "    \"hyperlink\": re.compile(r\"(http|www|\\.com)\"),\n",
        "    \"currency\": re.compile(r\"[$£€]\"),\n",
        "    \"non_alnum\": re.compile(r\"[^a-zA-Z0-9\\s]\"),\n",
        "    \"digit\": re.compile(r\"\\d\"),\n",
        "    \"upper_word\": re.compile(r\"\\b[A-Z]{2,}\\b\"),\n",
        "    \"repeat_char\": re.compile(r\"(.)\\1{2,}\"),\n",
        "}\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess_text(text):\n",
        "    if not isinstance(text, str): return \"\"\n",
        "    text = PATTERNS[\"url\"].sub(\" \", text)\n",
        "    text = PATTERNS[\"email\"].sub(\" \", text)\n",
        "    text = PATTERNS[\"non_alnum\"].sub(\" \", text).lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    return \" \".join([lemmatizer.lemmatize(t) for t in tokens if t not in stop_words and len(t) > 1])"
      ],
      "metadata": {
        "id": "iZ1tECd4I5Wv"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FEATURE ENGINEERING FUNCTIONS"
      ],
      "metadata": {
        "id": "922Kvb3FpV85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper for safe division\n",
        "def safe_division(numerator, denominator):\n",
        "    \"\"\"Performs division, returning 0 if the denominator is 0.\"\"\"\n",
        "    # Clip denominator at 1 to avoid division by zero\n",
        "    denom_clipped = denominator.clip(lower=1)\n",
        "    return numerator / denom_clipped\n",
        "\n",
        "def create_features(df, spam_vocab=None, mode='full'):\n",
        "    \"\"\"\n",
        "    Generates a set of features from the input text data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame containing 'text' and 'processed_text' columns.\n",
        "        spam_vocab (set, optional): A set of spam-related words. Defaults to None.\n",
        "        mode (str, optional): 'full' or 'streamlined'. Determines the feature set size.\n",
        "                            Defaults to 'full'.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The original DataFrame with new feature columns added.\n",
        "    \"\"\"\n",
        "    features_df = df.copy()\n",
        "    raw_text = features_df[\"text\"].astype(str)\n",
        "    processed_tokens = features_df[\"processed_text\"].str.split().fillna(\"\").apply(list)\n",
        "\n",
        "    # --- Core features (used in both modes) ---\n",
        "    features_df[\"feat_char_count\"] = raw_text.str.len()\n",
        "    features_df[\"feat_word_count\"] = processed_tokens.str.len()\n",
        "    features_df[\"feat_hyperlink_count\"] = raw_text.str.lower().str.count(PATTERNS[\"hyperlink\"])\n",
        "    features_df[\"feat_digit_count\"] = raw_text.str.count(PATTERNS[\"digit\"])\n",
        "\n",
        "    # Calculate spam word count if vocab is provided\n",
        "    if spam_vocab:\n",
        "        spam_word_count = processed_tokens.apply(lambda tokens: sum(1 for word in tokens if word in spam_vocab))\n",
        "        features_df[\"feat_spam_word_count\"] = spam_word_count\n",
        "\n",
        "    # --- Mode-specific features ---\n",
        "    if mode == 'full':\n",
        "        # More detailed linguistic features\n",
        "        features_df[\"feat_sentence_count\"] = raw_text.apply(lambda x: len(sent_tokenize(x))).clip(lower=1)\n",
        "        features_df[\"feat_paragraph_count\"] = raw_text.str.count(r'\\n\\n') + 1\n",
        "        features_df[\"feat_word_diversity\"] = processed_tokens.apply(lambda x: len(set(x)))\n",
        "        features_df[\"feat_uppercase_char_count\"] = raw_text.str.count(r\"[A-Z]\")\n",
        "\n",
        "        # Ratio-based features\n",
        "        features_df[\"feat_avg_word_len\"] = safe_division(raw_text.str.replace(\" \", \"\").str.len(), features_df[\"feat_word_count\"])\n",
        "        features_df[\"feat_word_diversity_ratio\"] = safe_division(features_df[\"feat_word_diversity\"], features_df[\"feat_word_count\"])\n",
        "        features_df[\"feat_uppercase_char_ratio\"] = safe_division(features_df[\"feat_uppercase_char_count\"], features_df[\"feat_char_count\"])\n",
        "        features_df[\"feat_word_per_sentence\"] = safe_division(features_df[\"feat_word_count\"], features_df[\"feat_sentence_count\"])\n",
        "        features_df[\"feat_word_per_paragraph\"] = safe_division(features_df[\"feat_word_count\"], features_df[\"feat_paragraph_count\"])\n",
        "        features_df[\"feat_sentence_per_paragraph\"] = safe_division(features_df[\"feat_sentence_count\"], features_df[\"feat_paragraph_count\"])\n",
        "        if spam_vocab:\n",
        "            features_df[\"feat_spam_word_ratio\"] = safe_division(features_df[\"feat_spam_word_count\"], features_df[\"feat_word_count\"])\n",
        "\n",
        "    elif mode == 'streamlined':\n",
        "        # Lightweight, faster-to-compute features\n",
        "        features_df[\"feat_avg_word_len\"] = safe_division(features_df[\"feat_char_count\"], features_df[\"feat_word_count\"])\n",
        "        features_df[\"feat_uppercase_word_count\"] = raw_text.str.count(PATTERNS[\"upper_word\"])\n",
        "        if spam_vocab:\n",
        "            features_df[\"feat_spam_word_ratio\"] = safe_division(spam_word_count, features_df[\"feat_word_count\"])\n",
        "\n",
        "    # Final cleanup\n",
        "    feature_cols = [c for c in features_df.columns if c.startswith(\"feat_\")]\n",
        "    features_df[feature_cols] = features_df[feature_cols].fillna(0)\n",
        "    return features_df"
      ],
      "metadata": {
        "id": "ByJpP7D2omJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STACKING EXPERIMENT RUNNER"
      ],
      "metadata": {
        "id": "DgT86ZXdiQe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_level_one_features(X_base_train, y_train, X_base_test, base_estimators):\n",
        "    \"\"\"\n",
        "    Trains base models and generates out-of-fold predictions (Level 1 features).\n",
        "    \"\"\"\n",
        "    print(\"--- Generating Level 1 features from base models... ---\")\n",
        "    oof_train_preds = []\n",
        "    test_preds = []\n",
        "    trained_base_models = {}\n",
        "\n",
        "    for name, model_pipeline in base_estimators:\n",
        "        # Generate out-of-fold predictions for the training set\n",
        "        oof_preds = cross_val_predict(\n",
        "            model_pipeline, X_base_train, y_train, cv=3, method='predict_proba', n_jobs=-1\n",
        "        )[:, 1]\n",
        "        oof_train_preds.append(pd.Series(oof_preds, name=f\"pred_{name}\", index=X_base_train.index))\n",
        "\n",
        "        # Train model on full training data to predict on the test set\n",
        "        model_pipeline.fit(X_base_train, y_train)\n",
        "        trained_base_models[name] = model_pipeline  # Save the trained model\n",
        "        test_p = model_pipeline.predict_proba(X_base_test)[:, 1]\n",
        "        test_preds.append(pd.Series(test_p, name=f\"pred_{name}\", index=X_base_test.index))\n",
        "\n",
        "    # Combine predictions into DataFrames\n",
        "    X_meta_train_preds = pd.concat(oof_train_preds, axis=1)\n",
        "    X_meta_test_preds = pd.concat(test_preds, axis=1)\n",
        "\n",
        "    return X_meta_train_preds, X_meta_test_preds, trained_base_models\n",
        "\n",
        "\n",
        "def train_and_evaluate_meta_model(X_meta_train, y_train, X_meta_test, y_test):\n",
        "    \"\"\"\n",
        "    Trains and evaluates the final meta-model.\n",
        "    \"\"\"\n",
        "    print(\"--- Training and evaluating final meta-model... ---\")\n",
        "\n",
        "    # Define the preprocessor for the meta-model\n",
        "    numeric_features = [c for c in X_meta_train.columns if c.startswith('feat_') or c.startswith('pred_')]\n",
        "    meta_preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('tfidf', TfidfVectorizer(max_features=2500), 'processed_text'),\n",
        "            ('numeric', StandardScaler(), numeric_features)\n",
        "        ],\n",
        "        remainder='drop'\n",
        "    )\n",
        "\n",
        "    # Define and train the meta-model\n",
        "    meta_model = Pipeline([\n",
        "        ('preprocessor', meta_preprocessor),\n",
        "        ('classifier', LogisticRegression(random_state=42, solver='liblinear'))\n",
        "    ])\n",
        "    meta_model.fit(X_meta_train, y_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = meta_model.predict(X_meta_test)\n",
        "    y_proba = meta_model.predict_proba(X_meta_test)[:, 1]\n",
        "\n",
        "    # Calculate metrics more efficiently\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    results = {\n",
        "        'Accuracy': report['accuracy'],\n",
        "        'Spam F1-Score': report['1']['f1-score'],\n",
        "        'MCC': matthews_corrcoef(y_test, y_pred),\n",
        "        'AUC': roc_auc_score(y_test, y_proba)\n",
        "    }\n",
        "\n",
        "    return meta_model, results\n",
        "\n",
        "# The main experiment function now orchestrates the calls\n",
        "def run_manual_stacking_experiment(df_base, df_meta, y, experiment_name, results_list, saved_models_dict):\n",
        "    \"\"\"\n",
        "    Performs a complete manual stacking run for a given configuration of features.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'#'*80}\\n# Running Stacking Experiment: {experiment_name}\\n{'#'*80}\")\n",
        "\n",
        "    # 1. Split data (ensure consistent splits)\n",
        "    X_base_train, X_base_test, y_train, y_test = train_test_split(df_base, y, test_size=0.25, random_state=42, stratify=y)\n",
        "    X_meta_train, X_meta_test, _, _ = train_test_split(df_meta, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "    # 2. Define base models\n",
        "    base_numeric_feats = [c for c in X_base_train.columns if c.startswith('feat_')]\n",
        "    base_preprocessor = ColumnTransformer(\n",
        "        [('tfidf', TfidfVectorizer(max_features=2500), 'processed_text'),\n",
        "         ('numeric', StandardScaler(), base_numeric_feats)],\n",
        "        remainder='drop'\n",
        "    )\n",
        "    base_estimators = [\n",
        "        ('lgbm', Pipeline([('preprocessor', base_preprocessor), ('clf', lgb.LGBMClassifier(random_state=42))])),\n",
        "        ('rf', Pipeline([('preprocessor', base_preprocessor), ('clf', RandomForestClassifier(random_state=42))])),\n",
        "    ]\n",
        "\n",
        "    # 3. Generate Level 1 features\n",
        "    X_meta_train_preds, X_meta_test_preds, trained_base_models = get_level_one_features(\n",
        "        X_base_train, y_train, X_base_test, base_estimators\n",
        "    )\n",
        "\n",
        "    # 4. Create final meta-feature sets by combining original meta features with predictions\n",
        "    X_meta_train_final = pd.concat([X_meta_train, X_meta_train_preds], axis=1)\n",
        "    X_meta_test_final = pd.concat([X_meta_test, X_meta_test_preds], axis=1)\n",
        "\n",
        "    # 5. Train and evaluate the meta-model\n",
        "    meta_model, result = train_and_evaluate_meta_model(\n",
        "        X_meta_train_final, y_train, X_meta_test_final, y_test\n",
        "    )\n",
        "\n",
        "    # 6. Store results and models\n",
        "    result['Experiment'] = experiment_name\n",
        "    results_list.append(result)\n",
        "    saved_models_dict[experiment_name] = {\n",
        "        'base_models': trained_base_models,\n",
        "        'meta_model': meta_model\n",
        "    }"
      ],
      "metadata": {
        "id": "n0ThLuiqqFvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # Path to file in Google Drive\n",
        "    file_path = '/content/drive/My Drive/spam_Emails_data.csv'\n",
        "\n",
        "    # Load CSV file into a pandas DataFrame (DF)\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.dropna(subset=['label'], inplace=True)\n",
        "    df['label'] = df['label'].str.strip().str.capitalize().map({'Ham': 0, 'Spam': 1})\n",
        "    df.dropna(subset=['label'], inplace=True)\n",
        "    df['label'] = df['label'].astype(int)\n",
        "    df['processed_text'] = df['text'].progress_apply(preprocess_text)\n",
        "    y = df['label']\n",
        "\n",
        "    temp_train_df, _ = train_test_split(df, test_size=0.3, random_state=42, stratify=y)\n",
        "    tfidf_vocab_gen = TfidfVectorizer(max_features=50, stop_words='english')\n",
        "    # Fit the vectorizer to your training text to build the vocabulary\n",
        "    tfidf_vocab_gen.fit(temp_train_df['processed_text'])\n",
        "    # Now that the vectorizer is fitted, you can safely get the vocabulary\n",
        "    DATA_DRIVEN_SPAM_VOCAB = set(tfidf_vocab_gen.get_feature_names_out())\n",
        "\n",
        "\n",
        "    # Create Both Feature Sets\n",
        "    print(\"\\n--- Creating feature sets for comparison... ---\")\n",
        "    feature_sets = {\n",
        "        \"Streamlined\": create_features(df.drop('label', axis=1), spam_vocab=DATA_DRIVEN_SPAM_VOCAB, mode='streamlined'),\n",
        "        \"Full\": create_features(df.drop('label', axis=1), spam_vocab=DATA_DRIVEN_SPAM_VOCAB, mode='full')\n",
        "    }\n",
        "\n",
        "\n",
        "    # Run all 4 Experiments\n",
        "    experiments_to_run = [\n",
        "        {'base': \"Streamlined\", 'meta': \"Streamlined\"},\n",
        "        {'base': \"Streamlined\", 'meta': \"Full\"},\n",
        "        {'base': \"Full\", 'meta': \"Streamlined\"},\n",
        "        {'base': \"Full\", 'meta': \"Full\"},\n",
        "    ]\n",
        "\n",
        "    all_results = []\n",
        "    all_trained_models = {}\n",
        "\n",
        "    for exp_config in experiments_to_run:\n",
        "        base_name = exp_config['base']\n",
        "        meta_name = exp_config['meta']\n",
        "        exp_name = f\"Base: {base_name} -> Meta: {meta_name}\"\n",
        "\n",
        "        run_manual_stacking_experiment(\n",
        "            df_base=feature_sets[base_name],\n",
        "            df_meta=feature_sets[meta_name],\n",
        "            y=y,\n",
        "            experiment_name=exp_name,\n",
        "            results_list=all_results,\n",
        "            saved_models_dict=all_trained_models\n",
        "        )\n",
        "\n",
        "    # Display Final Comparison Table\n",
        "    print(f\"\\n{'#'*80}\\n# FINAL STACKING EXPERIMENT SUMMARY\\n{'#'*80}\")\n",
        "    results_df = pd.DataFrame(all_results).sort_values(by='AUC', ascending=False).reset_index(drop=True)\n",
        "    print(results_df.to_string())\n",
        "\n",
        "    # Save Artifacts for the BEST Performing Experiment\n",
        "    BASE_PROJECT_PATH = '/content/drive/My Drive/SpamClassifierProject_AdvancedStacking'\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    RUN_SPECIFIC_PATH = os.path.join(BASE_PROJECT_PATH, f\"run_{timestamp}\")\n",
        "    os.makedirs(RUN_SPECIFIC_PATH, exist_ok=True)\n",
        "\n",
        "    print(f\"Artifacts will be saved in: {RUN_SPECIFIC_PATH}\")\n",
        "\n",
        "    # Identify best experiment\n",
        "    best_experiment_name = results_df.iloc[0]['Experiment']\n",
        "    best_model_stack = all_trained_models[best_experiment_name]\n",
        "\n",
        "    print(f\"Best performing experiment was: '{best_experiment_name}'\")\n",
        "\n",
        "    # Save each component of the best stack\n",
        "    for name, model in best_model_stack['base_models'].items():\n",
        "        joblib.dump(model, os.path.join(RUN_SPECIFIC_PATH, f'best_base_model_{name}.joblib'))\n",
        "    joblib.dump(best_model_stack['meta_model'], os.path.join(RUN_SPECIFIC_PATH, 'best_meta_model.joblib'))\n",
        "\n",
        "    # Save the summary & vocab\n",
        "    results_df.to_csv(os.path.join(RUN_SPECIFIC_PATH, 'experiment_summary.csv'), index=False)\n",
        "    joblib.dump(DATA_DRIVEN_SPAM_VOCAB, os.path.join(RUN_SPECIFIC_PATH, 'spam_vocabulary.joblib'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tehP2S-QMW-h",
        "outputId": "89913a49-3a23-4a70-c5a9-7b886f5f6974"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Creating feature sets for comparison... ---\n",
            "\n",
            "################################################################################\n",
            "# Running Stacking Experiment: Base: Streamlined -> Meta: Streamlined\n",
            "################################################################################\n",
            "--- Generating Level 1 features from base models... ---\n",
            "[LightGBM] [Info] Number of positive: 68769, number of negative: 76620\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.491067 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 623916\n",
            "[LightGBM] [Info] Number of data points in the train set: 145389, number of used features: 2506\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.473000 -> initscore=-0.108105\n",
            "[LightGBM] [Info] Start training from score -0.108105\n",
            "--- Training and evaluating final meta-model... ---\n",
            "\n",
            "################################################################################\n",
            "# Running Stacking Experiment: Base: Streamlined -> Meta: Full\n",
            "################################################################################\n",
            "--- Generating Level 1 features from base models... ---\n",
            "[LightGBM] [Info] Number of positive: 68769, number of negative: 76620\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.253020 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 623916\n",
            "[LightGBM] [Info] Number of data points in the train set: 145389, number of used features: 2506\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.473000 -> initscore=-0.108105\n",
            "[LightGBM] [Info] Start training from score -0.108105\n",
            "--- Training and evaluating final meta-model... ---\n",
            "\n",
            "################################################################################\n",
            "# Running Stacking Experiment: Base: Full -> Meta: Streamlined\n",
            "################################################################################\n",
            "--- Generating Level 1 features from base models... ---\n",
            "[LightGBM] [Info] Number of positive: 68769, number of negative: 76620\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.290547 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 625695\n",
            "[LightGBM] [Info] Number of data points in the train set: 145389, number of used features: 2513\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.473000 -> initscore=-0.108105\n",
            "[LightGBM] [Info] Start training from score -0.108105\n",
            "--- Training and evaluating final meta-model... ---\n",
            "\n",
            "################################################################################\n",
            "# Running Stacking Experiment: Base: Full -> Meta: Full\n",
            "################################################################################\n",
            "--- Generating Level 1 features from base models... ---\n",
            "[LightGBM] [Info] Number of positive: 68769, number of negative: 76620\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.269946 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 625695\n",
            "[LightGBM] [Info] Number of data points in the train set: 145389, number of used features: 2513\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.473000 -> initscore=-0.108105\n",
            "[LightGBM] [Info] Start training from score -0.108105\n",
            "--- Training and evaluating final meta-model... ---\n",
            "\n",
            "################################################################################\n",
            "# FINAL STACKING EXPERIMENT SUMMARY\n",
            "################################################################################\n",
            "                               Experiment  Accuracy  Spam F1-Score       MCC       AUC\n",
            "0                Base: Full -> Meta: Full  0.988878       0.988241  0.977691  0.999248\n",
            "1         Base: Full -> Meta: Streamlined  0.988837       0.988197  0.977608  0.999246\n",
            "2         Base: Streamlined -> Meta: Full  0.988445       0.987780  0.976821  0.999163\n",
            "3  Base: Streamlined -> Meta: Streamlined  0.988486       0.987824  0.976904  0.999133\n",
            "\n",
            "--- Saving artifacts for the best performing model... ---\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Artifacts will be saved in: /content/drive/My Drive/SpamClassifierProject_AdvancedStacking/run_2025-06-20_13-00-23\n",
            "Best performing experiment was: 'Base: Full -> Meta: Full'\n",
            "All components of the best model stack have been saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SAVE ALL MODELS AND ARTIFACTS TO GOOGLE DRIVE"
      ],
      "metadata": {
        "id": "nzTcC065ipTY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Mount Google Drive ---\n",
        "# This will prompt for authorization if not already mounted.\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- Create a unique, timestamped directory for this entire run ---\n",
        "BASE_PROJECT_PATH = '/content/drive/My Drive/SpamClassifierProject_AdvancedStacking'\n",
        "timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "RUN_SPECIFIC_PATH = os.path.join(BASE_PROJECT_PATH, f\"run_{timestamp}\")\n",
        "os.makedirs(RUN_SPECIFIC_PATH, exist_ok=True)\n",
        "\n",
        "print(f\"All artifacts for this run will be saved in: {RUN_SPECIFIC_PATH}\")\n",
        "\n",
        "# --- Save the overall experiment summary CSV ---\n",
        "# This file compares all the models you just ran.\n",
        "summary_csv_path = os.path.join(RUN_SPECIFIC_PATH, '1_full_experiment_summary.csv')\n",
        "results_df.to_csv(summary_csv_path, index=False)\n",
        "print(f\"\\nSaved overall results summary to: {summary_csv_path}\")\n",
        "\n",
        "# --- Save the data-driven spam vocabulary ---\n",
        "# This is a key artifact used in feature engineering.\n",
        "vocab_path = os.path.join(RUN_SPECIFIC_PATH, '2_spam_vocabulary.joblib')\n",
        "joblib.dump(DATA_DRIVEN_SPAM_VOCAB, vocab_path)\n",
        "print(f\"Saved spam vocabulary to: {vocab_path}\")\n",
        "\n",
        "# --- Create a dedicated folder for all the model files ---\n",
        "ALL_MODELS_PATH = os.path.join(RUN_SPECIFIC_PATH, '3_all_trained_model_stacks')\n",
        "os.makedirs(ALL_MODELS_PATH, exist_ok=True)\n",
        "print(f\"\\nSaving individual model stacks to: {ALL_MODELS_PATH}\")\n",
        "\n",
        "# --- Loop through each experiment and save its entire model stack ---\n",
        "for experiment_name, model_stack in all_trained_models.items():\n",
        "    # Sanitize the experiment name to create a valid folder name\n",
        "    safe_folder_name = experiment_name.replace(\" -> \", \"_\").replace(\": \", \"_\").replace(\" \", \"\")\n",
        "    EXPERIMENT_FOLDER_PATH = os.path.join(ALL_MODELS_PATH, safe_folder_name)\n",
        "    os.makedirs(EXPERIMENT_FOLDER_PATH, exist_ok=True)\n",
        "\n",
        "    print(f\"\\n  Saving models for experiment: '{experiment_name}'\")\n",
        "\n",
        "    # Save each base model from the stack\n",
        "    for name, model in model_stack['base_models'].items():\n",
        "        model_filename = os.path.join(EXPERIMENT_FOLDER_PATH, f'base_model_{name}.joblib')\n",
        "        joblib.dump(model, model_filename)\n",
        "        print(f\"    - Saved base model: {name}\")\n",
        "\n",
        "    # Save the meta model from the stack\n",
        "    meta_model_filename = os.path.join(EXPERIMENT_FOLDER_PATH, 'meta_model.joblib')\n",
        "    joblib.dump(model_stack['meta_model'], meta_model_filename)\n",
        "    print(f\"    - Saved meta model\")\n",
        "\n",
        "print(\"\\n\\n{'='*20} ALL ARTIFACTS SAVED SUCCESSFULLY! {'='*20}\")\n",
        "print(f\"You can find everything in your Google Drive at: {RUN_SPECIFIC_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtvtIKwpcfEx",
        "outputId": "d5657e59-2053-4c1d-e29d-71d7a98a19d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Saving artifacts for ALL experiments... ---\n",
            "Mounted at /content/drive\n",
            "All artifacts for this run will be saved in: /content/drive/My Drive/SpamClassifierProject_AdvancedStacking/run_2025-06-20_13-06-14\n",
            "\n",
            "Saved overall results summary to: /content/drive/My Drive/SpamClassifierProject_AdvancedStacking/run_2025-06-20_13-06-14/1_full_experiment_summary.csv\n",
            "Saved spam vocabulary to: /content/drive/My Drive/SpamClassifierProject_AdvancedStacking/run_2025-06-20_13-06-14/2_spam_vocabulary.joblib\n",
            "\n",
            "Saving individual model stacks to: /content/drive/My Drive/SpamClassifierProject_AdvancedStacking/run_2025-06-20_13-06-14/3_all_trained_model_stacks\n",
            "\n",
            "  Saving models for experiment: 'Base: Streamlined -> Meta: Streamlined'\n",
            "    - Saved base model: lgbm\n",
            "    - Saved base model: rf\n",
            "    - Saved meta model\n",
            "\n",
            "  Saving models for experiment: 'Base: Streamlined -> Meta: Full'\n",
            "    - Saved base model: lgbm\n",
            "    - Saved base model: rf\n",
            "    - Saved meta model\n",
            "\n",
            "  Saving models for experiment: 'Base: Full -> Meta: Streamlined'\n",
            "    - Saved base model: lgbm\n",
            "    - Saved base model: rf\n",
            "    - Saved meta model\n",
            "\n",
            "  Saving models for experiment: 'Base: Full -> Meta: Full'\n",
            "    - Saved base model: lgbm\n",
            "    - Saved base model: rf\n",
            "    - Saved meta model\n",
            "\n",
            "\n",
            "{'='*20} ALL ARTIFACTS SAVED SUCCESSFULLY! {'='*20}\n",
            "You can find everything in your Google Drive at: /content/drive/My Drive/SpamClassifierProject_AdvancedStacking/run_2025-06-20_13-06-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More readable output"
      ],
      "metadata": {
        "id": "jB-bTpeBi8G_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Styling the Results DataFrame ---\n",
        "\n",
        "# Set a style for the entire notebook's pandas outputs (optional but nice)\n",
        "pd.set_option('display.precision', 4)\n",
        "\n",
        "# Define a function to highlight the top row (our best model)\n",
        "def highlight_best(s):\n",
        "    \"\"\"Highlights the entire row of the best performing model in light green.\"\"\"\n",
        "    # Since the dataframe is already sorted, the best model is always at index 0\n",
        "    is_max = s.index == 0\n",
        "    return ['background-color: #d4edda; color: #155724; font-weight: bold;' if v else '' for v in is_max]\n",
        "\n",
        "# Define the columns that contain numeric metrics we want to style\n",
        "metric_columns = ['Accuracy', 'Spam F1-Score', 'MCC', 'AUC']\n",
        "\n",
        "# Apply all the styling rules to the DataFrame\n",
        "styled_results = results_df.style \\\n",
        "    .apply(highlight_best, axis=1) \\\n",
        "    .background_gradient(cmap='Greens', subset=metric_columns) \\\n",
        "    .format(\"{:.4f}\", subset=metric_columns) \\\n",
        "    .set_caption(\" Stacking Model Performance Comparison\") \\\n",
        "    .set_properties(**{\n",
        "        'border': '1px solid #ddd',\n",
        "        'text-align': 'center',\n",
        "        'width': '150px'\n",
        "    }) \\\n",
        "    .set_table_styles([\n",
        "        {'selector': 'th', 'props': [('background-color', '#343a40'), ('color', 'white'), ('font-size', '14px')]},\n",
        "        {'selector': 'caption', 'props': [('color', 'black'), ('font-size', '20px'), ('font-weight', 'bold'), ('margin', '15px')]}\n",
        "    ])\n",
        "\n",
        "# Display the styled DataFrame\n",
        "# In a Colab/Jupyter environment, the last line being the object will render it.\n",
        "styled_results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "PYjOrCqvdT8G",
        "outputId": "bdbe8584-81f5-4719-ebd8-23d3599c9ca1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x78d2996e73d0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_e2566 th {\n",
              "  background-color: #343a40;\n",
              "  color: white;\n",
              "  font-size: 14px;\n",
              "}\n",
              "#T_e2566 caption {\n",
              "  color: black;\n",
              "  font-size: 20px;\n",
              "  font-weight: bold;\n",
              "  margin: 15px;\n",
              "}\n",
              "#T_e2566_row0_col0, #T_e2566_row1_col0, #T_e2566_row2_col0, #T_e2566_row3_col0 {\n",
              "  border: 1px solid #ddd;\n",
              "  text-align: center;\n",
              "  width: 150px;\n",
              "}\n",
              "#T_e2566_row0_col1, #T_e2566_row0_col2, #T_e2566_row0_col3, #T_e2566_row0_col4 {\n",
              "  background-color: #00441b;\n",
              "  color: #f1f1f1;\n",
              "  border: 1px solid #ddd;\n",
              "  text-align: center;\n",
              "  width: 150px;\n",
              "}\n",
              "#T_e2566_row1_col1, #T_e2566_row1_col2, #T_e2566_row1_col3 {\n",
              "  background-color: #006328;\n",
              "  color: #f1f1f1;\n",
              "  border: 1px solid #ddd;\n",
              "  text-align: center;\n",
              "  width: 150px;\n",
              "}\n",
              "#T_e2566_row1_col4 {\n",
              "  background-color: #00481d;\n",
              "  color: #f1f1f1;\n",
              "  border: 1px solid #ddd;\n",
              "  text-align: center;\n",
              "  width: 150px;\n",
              "}\n",
              "#T_e2566_row2_col1, #T_e2566_row2_col2, #T_e2566_row2_col3, #T_e2566_row3_col4 {\n",
              "  background-color: #f7fcf5;\n",
              "  color: #000000;\n",
              "  border: 1px solid #ddd;\n",
              "  text-align: center;\n",
              "  width: 150px;\n",
              "}\n",
              "#T_e2566_row2_col4 {\n",
              "  background-color: #c3e7bc;\n",
              "  color: #000000;\n",
              "  border: 1px solid #ddd;\n",
              "  text-align: center;\n",
              "  width: 150px;\n",
              "}\n",
              "#T_e2566_row3_col1, #T_e2566_row3_col2, #T_e2566_row3_col3 {\n",
              "  background-color: #e9f7e5;\n",
              "  color: #000000;\n",
              "  border: 1px solid #ddd;\n",
              "  text-align: center;\n",
              "  width: 150px;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_e2566\" class=\"dataframe\">\n",
              "  <caption>🏆 Stacking Model Performance Comparison 🏆</caption>\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_e2566_level0_col0\" class=\"col_heading level0 col0\" >Experiment</th>\n",
              "      <th id=\"T_e2566_level0_col1\" class=\"col_heading level0 col1\" >Accuracy</th>\n",
              "      <th id=\"T_e2566_level0_col2\" class=\"col_heading level0 col2\" >Spam F1-Score</th>\n",
              "      <th id=\"T_e2566_level0_col3\" class=\"col_heading level0 col3\" >MCC</th>\n",
              "      <th id=\"T_e2566_level0_col4\" class=\"col_heading level0 col4\" >AUC</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_e2566_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_e2566_row0_col0\" class=\"data row0 col0\" >Base: Full -> Meta: Full</td>\n",
              "      <td id=\"T_e2566_row0_col1\" class=\"data row0 col1\" >0.9889</td>\n",
              "      <td id=\"T_e2566_row0_col2\" class=\"data row0 col2\" >0.9882</td>\n",
              "      <td id=\"T_e2566_row0_col3\" class=\"data row0 col3\" >0.9777</td>\n",
              "      <td id=\"T_e2566_row0_col4\" class=\"data row0 col4\" >0.9992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e2566_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_e2566_row1_col0\" class=\"data row1 col0\" >Base: Full -> Meta: Streamlined</td>\n",
              "      <td id=\"T_e2566_row1_col1\" class=\"data row1 col1\" >0.9888</td>\n",
              "      <td id=\"T_e2566_row1_col2\" class=\"data row1 col2\" >0.9882</td>\n",
              "      <td id=\"T_e2566_row1_col3\" class=\"data row1 col3\" >0.9776</td>\n",
              "      <td id=\"T_e2566_row1_col4\" class=\"data row1 col4\" >0.9992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e2566_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_e2566_row2_col0\" class=\"data row2 col0\" >Base: Streamlined -> Meta: Full</td>\n",
              "      <td id=\"T_e2566_row2_col1\" class=\"data row2 col1\" >0.9884</td>\n",
              "      <td id=\"T_e2566_row2_col2\" class=\"data row2 col2\" >0.9878</td>\n",
              "      <td id=\"T_e2566_row2_col3\" class=\"data row2 col3\" >0.9768</td>\n",
              "      <td id=\"T_e2566_row2_col4\" class=\"data row2 col4\" >0.9992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_e2566_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_e2566_row3_col0\" class=\"data row3 col0\" >Base: Streamlined -> Meta: Streamlined</td>\n",
              "      <td id=\"T_e2566_row3_col1\" class=\"data row3 col1\" >0.9885</td>\n",
              "      <td id=\"T_e2566_row3_col2\" class=\"data row3 col2\" >0.9878</td>\n",
              "      <td id=\"T_e2566_row3_col3\" class=\"data row3 col3\" >0.9769</td>\n",
              "      <td id=\"T_e2566_row3_col4\" class=\"data row3 col4\" >0.9991</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HYPERPARAMETER TUNING WITH GRIDSEARCHCV (WITH CHECKPOINTS)"
      ],
      "metadata": {
        "id": "9lsck89QjDlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Step 1: Mount Drive and Setup Checkpoint Directory ---\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    # This directory is for saving progress and resuming if the session fails\n",
        "    CHECKPOINT_DIR = '/content/drive/My Drive/SpamClassifierProject_Checkpoints'\n",
        "    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
        "    print(f\"Checkpoint directory is ready at: {CHECKPOINT_DIR}\")\n",
        "\n",
        "except ImportError:\n",
        "    print(\"\\nCould not connect to Google Drive. Checkpointing is disabled.\")\n",
        "    CHECKPOINT_DIR = None\n",
        "except Exception as e:\n",
        "    print(f\"\\nAn error occurred during Google Drive setup: {e}\")\n",
        "    CHECKPOINT_DIR = None\n",
        "\n",
        "\n",
        "# --- Step 2: Define Hyperparameter Grids & Prepare Data ---\n",
        "# Grids are unchanged\n",
        "lgbm_param_grid = {\n",
        "    'clf__n_estimators': [100, 200], 'clf__learning_rate': [0.05, 0.1], 'clf__num_leaves': [31, 50]\n",
        "}\n",
        "rf_param_grid = {\n",
        "    'clf__n_estimators': [100, 200], 'clf__max_depth': [10, 20, None], 'clf__min_samples_split': [2, 5]\n",
        "}\n",
        "logistic_param_grid = {\n",
        "    'classifier__C': [0.1, 1.0, 10.0], 'classifier__penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "# Data setup is unchanged\n",
        "df_base_best = feature_sets[\"Full\"]\n",
        "df_meta_best = feature_sets[\"Full\"]\n",
        "X_base_train, X_base_test, y_train, y_test = train_test_split(\n",
        "    df_base_best, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "X_meta_train, X_meta_test, _, _ = train_test_split(\n",
        "    df_meta_best, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "\n",
        "# --- Step 3: Tune Base Models with Checkpointing ---\n",
        "print(\"\\n--- Tuning Base Models ---\")\n",
        "base_preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('tfidf', TfidfVectorizer(max_features=2500), 'processed_text'),\n",
        "        ('numeric', StandardScaler(), [c for c in X_base_train.columns if c.startswith('feat_')])\n",
        "    ], remainder='drop'\n",
        ")\n",
        "base_estimators_for_tuning = {\n",
        "    'lgbm': (Pipeline([('preprocessor', base_preprocessor), ('clf', lgb.LGBMClassifier(random_state=42))]), lgbm_param_grid),\n",
        "    'rf': (Pipeline([('preprocessor', base_preprocessor), ('clf', RandomForestClassifier(random_state=42))]), rf_param_grid)\n",
        "}\n",
        "\n",
        "best_base_estimators = {}\n",
        "best_base_params = {}\n",
        "\n",
        "for name, (pipeline, param_grid) in base_estimators_for_tuning.items():\n",
        "    checkpoint_path = os.path.join(CHECKPOINT_DIR, f'base_model_{name}_checkpoint.joblib')\n",
        "\n",
        "    if CHECKPOINT_DIR and os.path.exists(checkpoint_path):\n",
        "        # If checkpoint exists, load it\n",
        "        print(f\"Found checkpoint for {name}. Loading pre-tuned model.\")\n",
        "        checkpoint_data = joblib.load(checkpoint_path)\n",
        "        best_base_estimators[name] = checkpoint_data['estimator']\n",
        "        best_base_params[name] = checkpoint_data['params']\n",
        "        print(f\"Loaded best parameters for {name}: {best_base_params[name]}\\n\")\n",
        "    else:\n",
        "        # Otherwise, run GridSearchCV and save checkpoint\n",
        "        print(f\"No checkpoint found for {name}. Running GridSearchCV...\")\n",
        "        grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
        "        grid_search.fit(X_base_train, y_train)\n",
        "\n",
        "        best_base_estimators[name] = grid_search.best_estimator_\n",
        "        best_base_params[name] = grid_search.best_params_\n",
        "\n",
        "        print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
        "        print(f\"Best AUC score for {name}: {grid_search.best_score_:.4f}\\n\")\n",
        "\n",
        "        if CHECKPOINT_DIR:\n",
        "            print(f\"Saving checkpoint for {name} to Google Drive...\")\n",
        "            checkpoint_data = {'estimator': grid_search.best_estimator_, 'params': grid_search.best_params_}\n",
        "            joblib.dump(checkpoint_data, checkpoint_path)\n",
        "            print(\"Checkpoint saved.\\n\")\n",
        "\n",
        "\n",
        "# --- Step 4: Generate Level 1 Features (No Checkpoint Needed) ---\n",
        "print(\"--- Generating Level 1 features using tuned base models... ---\")\n",
        "# This step is fast and can be rerun without issue.\n",
        "oof_train_preds_tuned = []\n",
        "test_preds_tuned = []\n",
        "\n",
        "for name, best_model in best_base_estimators.items():\n",
        "    oof_preds = cross_val_predict(best_model, X_base_train, y_train, cv=3, method='predict_proba', n_jobs=-1)[:, 1]\n",
        "    oof_train_preds_tuned.append(pd.Series(oof_preds, name=f\"pred_{name}_tuned\", index=X_base_train.index))\n",
        "\n",
        "    test_p = best_model.predict_proba(X_base_test)[:, 1]\n",
        "    test_preds_tuned.append(pd.Series(test_p, name=f\"pred_{name}_tuned\", index=X_base_test.index))\n",
        "\n",
        "X_meta_train_final_tuned = pd.concat([X_meta_train] + oof_train_preds_tuned, axis=1)\n",
        "X_meta_test_final_tuned = pd.concat([X_meta_test] + test_preds_tuned, axis=1)\n",
        "print(\"Level 1 features generated.\")\n",
        "\n",
        "\n",
        "# --- Step 5: Tune the Meta-Model with Checkpointing ---\n",
        "print(\"\\n--- Tuning the Meta-Model ---\")\n",
        "meta_model_checkpoint_path = os.path.join(CHECKPOINT_DIR, 'meta_model_checkpoint.joblib')\n",
        "\n",
        "best_meta_model = None\n",
        "best_meta_params = None\n",
        "\n",
        "if CHECKPOINT_DIR and os.path.exists(meta_model_checkpoint_path):\n",
        "    print(\"Found checkpoint for Meta-Model. Loading pre-tuned model.\")\n",
        "    checkpoint_data = joblib.load(meta_model_checkpoint_path)\n",
        "    best_meta_model = checkpoint_data['estimator']\n",
        "    best_meta_params = checkpoint_data['params']\n",
        "    print(f\"Loaded best parameters for Meta-Model: {best_meta_params}\\n\")\n",
        "else:\n",
        "    print(\"No checkpoint found for Meta-Model. Running GridSearchCV...\")\n",
        "    meta_numeric_features_tuned = [c for c in X_meta_train_final_tuned.columns if c.startswith('feat_') or c.startswith('pred_')]\n",
        "    meta_preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('tfidf', TfidfVectorizer(max_features=2500), 'processed_text'),\n",
        "            ('numeric', StandardScaler(), meta_numeric_features_tuned)\n",
        "        ], remainder='drop'\n",
        "    )\n",
        "    meta_model_pipeline = Pipeline([\n",
        "        ('preprocessor', meta_preprocessor),\n",
        "        ('classifier', LogisticRegression(random_state=42, solver='liblinear'))\n",
        "    ])\n",
        "\n",
        "    meta_grid_search = GridSearchCV(meta_model_pipeline, logistic_param_grid, cv=3, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
        "    meta_grid_search.fit(X_meta_train_final_tuned, y_train)\n",
        "\n",
        "    best_meta_model = meta_grid_search.best_estimator_\n",
        "    best_meta_params = meta_grid_search.best_params_\n",
        "\n",
        "    print(f\"Best parameters for Meta-Model: {meta_grid_search.best_params_}\")\n",
        "    print(f\"Best AUC score for Meta-Model on training data: {meta_grid_search.best_score_:.4f}\\n\")\n",
        "\n",
        "    if CHECKPOINT_DIR:\n",
        "        print(\"Saving checkpoint for Meta-Model to Google Drive...\")\n",
        "        checkpoint_data = {'estimator': best_meta_model, 'params': best_meta_params}\n",
        "        joblib.dump(checkpoint_data, meta_model_checkpoint_path)\n",
        "        print(\"Checkpoint saved.\\n\")\n",
        "\n",
        "\n",
        "# --- Step 6: Evaluate the Final, Fully-Tuned Stacking Model ---\n",
        "print(f\"\\n{'#'*80}\\n# FINAL TUNED MODEL PERFORMANCE\\n{'#'*80}\")\n",
        "\n",
        "y_pred_tuned = best_meta_model.predict(X_meta_test_final_tuned)\n",
        "y_proba_tuned = best_meta_model.predict_proba(X_meta_test_final_tuned)[:, 1]\n",
        "\n",
        "report_str = classification_report(y_test, y_pred_tuned, target_names=['Ham (0)', 'Spam (1)'])\n",
        "report_dict = classification_report(y_test, y_pred_tuned, output_dict=True)\n",
        "\n",
        "print(\"Classification Report for the Tuned Model:\")\n",
        "print(report_str)\n",
        "\n",
        "final_metrics = {\n",
        "    'Tuned Accuracy': f\"{report_dict['accuracy']:.4f}\",\n",
        "    'Tuned Spam F1-Score': f\"{report_dict['1']['f1-score']:.4f}\",\n",
        "    'Tuned MCC': f\"{matthews_corrcoef(y_test, y_pred_tuned):.4f}\",\n",
        "    'Tuned AUC': f\"{roc_auc_score(y_test, y_proba_tuned):.4f}\"\n",
        "}\n",
        "\n",
        "print(\"\\nFinal Performance Metrics:\")\n",
        "for metric, value in final_metrics.items():\n",
        "    print(f\"- {metric}: {value}\")\n",
        "\n",
        "print(\"\\nHyperparameter tuning complete.\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# 7. SAVE FINAL TUNED MODEL ARTIFACTS\n",
        "# ==============================================================================\n",
        "# This section saves the final results to a NEW, timestamped folder for archival.\n",
        "if CHECKPOINT_DIR:\n",
        "    print(\"\\n--- Saving final artifacts for the tuned model stack... ---\")\n",
        "\n",
        "    BASE_PROJECT_PATH = '/content/drive/My Drive/SpamClassifierProject_TunedStacking'\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    RUN_SPECIFIC_PATH = os.path.join(BASE_PROJECT_PATH, f\"tuned_run_{timestamp}\")\n",
        "    os.makedirs(RUN_SPECIFIC_PATH, exist_ok=True)\n",
        "\n",
        "    print(f\"Final artifacts will be saved in: {RUN_SPECIFIC_PATH}\")\n",
        "\n",
        "    # Save each tuned base model from the 'best_base_estimators' dict\n",
        "    for name, model in best_base_estimators.items():\n",
        "        joblib.dump(model, os.path.join(RUN_SPECIFIC_PATH, f'tuned_base_model_{name}.joblib'))\n",
        "\n",
        "    # Save the tuned meta model\n",
        "    joblib.dump(best_meta_model, os.path.join(RUN_SPECIFIC_PATH, 'tuned_meta_model.joblib'))\n",
        "    print(\"Tuned base and meta models saved successfully.\")\n",
        "\n",
        "    summary_report = {\n",
        "        'Final Performance Metrics': final_metrics,\n",
        "        'Best Base Model Parameters': best_base_params,\n",
        "        'Best Meta Model Parameters': best_meta_params\n",
        "    }\n",
        "    with open(os.path.join(RUN_SPECIFIC_PATH, 'summary_report.json'), 'w') as f:\n",
        "        json.dump(summary_report, f, indent=4)\n",
        "    with open(os.path.join(RUN_SPECIFIC_PATH, 'classification_report.txt'), 'w') as f:\n",
        "        f.write(report_str)\n",
        "    print(\"Performance reports and parameters saved successfully.\")\n",
        "\n",
        "    joblib.dump(DATA_DRIVEN_SPAM_VOCAB, os.path.join(RUN_SPECIFIC_PATH, 'spam_vocabulary.joblib'))\n",
        "    print(\"Spam vocabulary saved successfully.\")\n",
        "\n",
        "    print(\"\\nAll final artifacts have been saved.\")\n",
        "else:\n",
        "    print(\"\\nGoogle Drive not connected. Skipping final artifact saving.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ImsQ7EffmYn",
        "outputId": "00c1326c-4929-4262-e541-1c8e3ccb53f3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Hyperparameter Tuning for the Best Model Stack ---\n",
            "Mounted at /content/drive\n",
            "Checkpoint directory is ready at: /content/drive/My Drive/SpamClassifierProject_Checkpoints\n",
            "\n",
            "--- Tuning Base Models ---\n",
            "No checkpoint found for lgbm. Running GridSearchCV...\n",
            "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
            "[LightGBM] [Info] Number of positive: 68769, number of negative: 76620\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.265260 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 625695\n",
            "[LightGBM] [Info] Number of data points in the train set: 145389, number of used features: 2513\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.473000 -> initscore=-0.108105\n",
            "[LightGBM] [Info] Start training from score -0.108105\n",
            "Best parameters for lgbm: {'clf__learning_rate': 0.1, 'clf__n_estimators': 200, 'clf__num_leaves': 50}\n",
            "Best AUC score for lgbm: 0.9984\n",
            "\n",
            "Saving checkpoint for lgbm to Google Drive...\n",
            "Checkpoint saved.\n",
            "\n",
            "No checkpoint found for rf. Running GridSearchCV...\n",
            "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
            "Best parameters for rf: {'clf__max_depth': None, 'clf__min_samples_split': 5, 'clf__n_estimators': 200}\n",
            "Best AUC score for rf: 0.9980\n",
            "\n",
            "Saving checkpoint for rf to Google Drive...\n",
            "Checkpoint saved.\n",
            "\n",
            "--- Generating Level 1 features using tuned base models... ---\n",
            "Level 1 features generated.\n",
            "\n",
            "--- Tuning the Meta-Model ---\n",
            "No checkpoint found for Meta-Model. Running GridSearchCV...\n",
            "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
            "Best parameters for Meta-Model: {'classifier__C': 1.0, 'classifier__penalty': 'l2'}\n",
            "Best AUC score for Meta-Model on training data: 0.9988\n",
            "\n",
            "Saving checkpoint for Meta-Model to Google Drive...\n",
            "Checkpoint saved.\n",
            "\n",
            "\n",
            "################################################################################\n",
            "# FINAL TUNED MODEL PERFORMANCE\n",
            "################################################################################\n",
            "Classification Report for the Tuned Model:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Ham (0)       0.99      0.99      0.99     25540\n",
            "    Spam (1)       0.99      0.99      0.99     22923\n",
            "\n",
            "    accuracy                           0.99     48463\n",
            "   macro avg       0.99      0.99      0.99     48463\n",
            "weighted avg       0.99      0.99      0.99     48463\n",
            "\n",
            "\n",
            "Final Performance Metrics:\n",
            "- Tuned Accuracy: 0.9895\n",
            "- Tuned Spam F1-Score: 0.9889\n",
            "- Tuned MCC: 0.9789\n",
            "- Tuned AUC: 0.9993\n",
            "\n",
            "Hyperparameter tuning complete.\n",
            "\n",
            "--- Saving final artifacts for the tuned model stack... ---\n",
            "Final artifacts will be saved in: /content/drive/My Drive/SpamClassifierProject_TunedStacking/tuned_run_2025-06-20_15-13-36\n",
            "Tuned base and meta models saved successfully.\n",
            "Performance reports and parameters saved successfully.\n",
            "Spam vocabulary saved successfully.\n",
            "\n",
            "All final artifacts have been saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Assistance Disclosure:**\n",
        "\n",
        "I used LLMs (Codey, ChatGPT, Gemini, Claude, Grok) for brainstorming, debugging, feedback, and improving code readability."
      ],
      "metadata": {
        "id": "3Ruumq9PnD4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# To Do: Fuller list of functions to try"
      ],
      "metadata": {
        "id": "GGITzEYRoEgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fuller list of functions:\n",
        "    features_df[\"feat_char_count\"] = raw_text.str.len()\n",
        "    features_df[\"feat_word_count\"] = processed_tokens.str.len()\n",
        "    features_df[\"feat_sentence_count\"] = raw_text.apply(lambda x: len(sent_tokenize(x))).clip(lower=1)\n",
        "    features_df[\"feat_paragraph_count\"] = raw_text.str.count(r'\\n\\n') + 1\n",
        "    features_df[\"feat_word_diversity\"] = processed_tokens.apply(lambda x: len(set(x)))\n",
        "    if spam_vocab:\n",
        "        features_df[\"feat_spam_word_count\"] = processed_tokens.apply(lambda t: sum(1 for w in t if w in spam_vocab))\n",
        "    features_df[\"feat_avg_word_len\"] = raw_text.str.split().str.join('').str.len() / features_df[\"feat_word_count\"].clip(lower=1)\n",
        "    features_df[\"feat_word_diversity_ratio\"] = features_df[\"feat_word_diversity\"] / features_df[\"feat_word_count\"].clip(lower=1)\n",
        "    features_df[\"feat_uppercase_char_ratio\"] = raw_text.str.count(r\"[A-Z]\") / features_df[\"feat_char_count\"].clip(lower=1)\n",
        "    features_df[\"feat_word_per_sentence\"] = features_df[\"feat_word_count\"] / features_df[\"feat_sentence_count\"].clip(lower=1)\n",
        "    features_df[\"feat_word_per_paragraph\"] = features_df[\"feat_word_count\"] / features_df[\"feat_paragraph_count\"].clip(lower=1)\n",
        "    features_df[\"feat_sentence_per_paragraph\"] = features_df[\"feat_sentence_count\"] / features_df[\"feat_paragraph_count\"].clip(lower=1)\n",
        "    features_df[\"feat_unique_word_per_sentence\"] = features_df[\"feat_word_diversity\"] / features_df[\"feat_sentence_count\"].clip(lower=1)\n",
        "    features_df[\"feat_unique_word_per_paragraph\"] = features_df[\"feat_word_diversity\"] / features_df[\"feat_paragraph_count\"].clip(lower=1)\n",
        "    features_df[\"feat_hyperlink_count\"] = raw_text.str.lower().str.count(PATTERNS[\"hyperlink\"])\n",
        "    features_df[\"feat_exclamation_count\"] = raw_text.str.count(\"!\")\n",
        "    features_df[\"feat_question_count\"] = raw_text.str.count(r\"\\?\")\n",
        "    features_df[\"feat_digit_count\"] = raw_text.str.count(PATTERNS[\"digit\"])\n",
        "    features_df[\"feat_uppercase_word_count\"] = raw_text.str.count(PATTERNS[\"upper_word\"])\n",
        "    features_df[\"feat_special_char_count\"] = raw_text.str.count(fr\"[{re.escape(string.punctuation)}]\")\n",
        "    features_df[\"feat_currency_symbol_count\"] = raw_text.str.count(PATTERNS[\"currency\"])\n",
        "    features_df[\"feat_phone_pattern_count\"] = raw_text.str.count(PATTERNS[\"phone\"])\n",
        "    features_df[\"feat_repeat_char_count\"] = raw_text.str.count(PATTERNS[\"repeat_char\"])\n",
        "    if spam_vocab:\n",
        "        features_df[\"feat_spam_word_ratio\"] = features_df[\"feat_spam_word_count\"] / features_df[\"feat_word_count\"].clip(lower=1)\n",
        "        features_df[\"feat_spam_word_per_sentence\"] = features_df[\"feat_spam_word_count\"] / features_df[\"feat_sentence_count\"].clip(lower=1)\n",
        "        features_df[\"feat_spam_word_per_paragraph\"] = features_df[\"feat_spam_word_count\"] / features_df[\"feat_paragraph_count\"].clip(lower=1)\n",
        "        features_df[\"feat_spam_word_per_unique_word\"] = features_df[\"feat_spam_word_count\"] / features_df[\"feat_word_diversity\"].clip(lower=1)\n"
      ],
      "metadata": {
        "id": "guKT2aeRnz7v"
      }
    }
  ]
}